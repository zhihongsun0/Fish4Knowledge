{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time,os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\") \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fish_01': 0, 'fish_02': 1, 'fish_03': 2, 'fish_04': 3, 'fish_05': 4}\n",
      "{'fish_01': 0, 'fish_02': 1, 'fish_03': 2, 'fish_04': 3, 'fish_05': 4}\n",
      "{'fish_01': 0, 'fish_02': 1, 'fish_03': 2, 'fish_04': 3, 'fish_05': 4}\n"
     ]
    }
   ],
   "source": [
    "# setting dataset path\n",
    "root_dir  = os.path.join(\"data\", \"split\")\n",
    "path_dict = {\n",
    "    'train': os.path.join(root_dir, \"train\"),\n",
    "    \"valid\": os.path.join(root_dir, \"valid\"),\n",
    "    \"test\":  os.path.join(root_dir, \"test\")\n",
    "}\n",
    "\n",
    "\n",
    "# setting transform\n",
    "transform_dict = {\n",
    "    'train': torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.Resize(72),\n",
    "    torchvision.transforms.RandomResizedCrop(64,(0.5, 1.0), ratio=(0.75, 1.3333333333333333)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "]),\n",
    "    \"valid\": torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(72),\n",
    "    torchvision.transforms.CenterCrop(64),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "]),\n",
    "    \"test\": torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(72),\n",
    "    torchvision.transforms.CenterCrop(64),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])}\n",
    "\n",
    "train_set = torchvision.datasets.ImageFolder(path_dict['train'],transform_dict['train'])\n",
    "valid_set = torchvision.datasets.ImageFolder(path_dict['valid'],transform_dict['valid'])\n",
    "test_set  = torchvision.datasets.ImageFolder(path_dict['test'], transform_dict['test'])\n",
    "\n",
    "print(valid_set.class_to_idx)\n",
    "print(train_set.class_to_idx)\n",
    "print(test_set.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper_para\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "# setting loader\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)   \n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)   \n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# considering the num of train set is limited, we adopt a rather small CNN-network:modified lenet\n",
    "class CNN_LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_LeNet, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, 5, 1, 2), # in_channels, out_channels, kernel_size\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # kernel_size, stride\n",
    "            nn.Conv2d(6, 16, 5, 2, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 5, 2, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(32*4*4, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, 5)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        feature = self.conv(img)\n",
    "        output = self.fc(feature.view(feature.size()[0], -1))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_LeNet(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (4): ReLU()\n",
       "    (5): Conv2d(16, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=120, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=84, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_LeNet = CNN_LeNet()\n",
    "my_LeNet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-para\n",
    "EPOCH = 20\n",
    "LR_LeNet = 0.001\n",
    "\n",
    "# print setting\n",
    "print_freq = 10\n",
    "drop_lr_after_epoch_num = 2\n",
    "\n",
    "\n",
    "# training setting\n",
    "optimizer_LeNet = optim.AdamW(my_LeNet.parameters(), LR_LeNet)\n",
    "criterion_LeNet = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training, LeNet!\n",
      "[1, 10] loss: 1.469\n",
      "[1, 20] loss: 1.221\n",
      "[1, 30] loss: 1.020\n",
      "[1, 40] loss: 0.855\n",
      "accuracy on valid set on epoch 1 ：64.303%\n",
      "Change learning rate to : 0.000100000 at epoch 2\n",
      "[2, 10] loss: 0.836\n",
      "[2, 20] loss: 0.719\n",
      "[2, 30] loss: 0.630\n",
      "[2, 40] loss: 0.534\n",
      "accuracy on valid set on epoch 2 ：81.771%\n",
      "[3, 10] loss: 0.537\n",
      "[3, 20] loss: 0.469\n",
      "[3, 30] loss: 0.440\n",
      "[3, 40] loss: 0.410\n",
      "accuracy on valid set on epoch 3 ：87.780%\n",
      "Change learning rate to : 0.000010000 at epoch 4\n",
      "[4, 10] loss: 0.393\n",
      "[4, 20] loss: 0.376\n",
      "[4, 30] loss: 0.358\n",
      "[4, 40] loss: 0.336\n",
      "accuracy on valid set on epoch 4 ：89.583%\n",
      "[5, 10] loss: 0.337\n",
      "[5, 20] loss: 0.339\n",
      "[5, 30] loss: 0.322\n",
      "[5, 40] loss: 0.299\n",
      "accuracy on valid set on epoch 5 ：87.500%\n",
      "Change learning rate to : 0.000001000 at epoch 6\n",
      "[6, 10] loss: 0.334\n",
      "[6, 20] loss: 0.300\n",
      "[6, 30] loss: 0.289\n",
      "[6, 40] loss: 0.262\n",
      "accuracy on valid set on epoch 6 ：90.465%\n",
      "[7, 10] loss: 0.271\n",
      "[7, 20] loss: 0.264\n",
      "[7, 30] loss: 0.263\n",
      "[7, 40] loss: 0.248\n",
      "accuracy on valid set on epoch 7 ：87.460%\n",
      "Change learning rate to : 0.000000100 at epoch 8\n",
      "[8, 10] loss: 0.272\n",
      "[8, 20] loss: 0.258\n",
      "[8, 30] loss: 0.248\n",
      "[8, 40] loss: 0.273\n",
      "accuracy on valid set on epoch 8 ：86.619%\n",
      "[9, 10] loss: 0.355\n",
      "[9, 20] loss: 0.275\n",
      "[9, 30] loss: 0.251\n",
      "[9, 40] loss: 0.257\n",
      "accuracy on valid set on epoch 9 ：92.628%\n",
      "Change learning rate to : 0.000000010 at epoch 10\n",
      "[10, 10] loss: 0.237\n",
      "[10, 20] loss: 0.220\n",
      "[10, 30] loss: 0.218\n",
      "[10, 40] loss: 0.196\n",
      "accuracy on valid set on epoch 10 ：92.708%\n",
      "[11, 10] loss: 0.206\n",
      "[11, 20] loss: 0.193\n",
      "[11, 30] loss: 0.215\n",
      "[11, 40] loss: 0.212\n",
      "accuracy on valid set on epoch 11 ：90.064%\n",
      "Change learning rate to : 0.000000001 at epoch 12\n",
      "[12, 10] loss: 0.301\n",
      "[12, 20] loss: 0.236\n",
      "[12, 30] loss: 0.225\n",
      "[12, 40] loss: 0.203\n",
      "accuracy on valid set on epoch 12 ：92.708%\n",
      "[13, 10] loss: 0.220\n",
      "[13, 20] loss: 0.209\n",
      "[13, 30] loss: 0.212\n",
      "[13, 40] loss: 0.172\n",
      "accuracy on valid set on epoch 13 ：93.750%\n",
      "Change learning rate to : 0.000000000 at epoch 14\n",
      "[14, 10] loss: 0.178\n",
      "[14, 20] loss: 0.178\n",
      "[14, 30] loss: 0.172\n",
      "[14, 40] loss: 0.222\n",
      "accuracy on valid set on epoch 14 ：94.431%\n",
      "[15, 10] loss: 0.203\n",
      "[15, 20] loss: 0.191\n",
      "[15, 30] loss: 0.182\n",
      "[15, 40] loss: 0.169\n",
      "accuracy on valid set on epoch 15 ：93.510%\n",
      "Change learning rate to : 0.000000000 at epoch 16\n",
      "[16, 10] loss: 0.179\n",
      "[16, 20] loss: 0.161\n",
      "[16, 30] loss: 0.152\n",
      "[16, 40] loss: 0.148\n",
      "accuracy on valid set on epoch 16 ：95.353%\n",
      "[17, 10] loss: 0.145\n",
      "[17, 20] loss: 0.161\n",
      "[17, 30] loss: 0.161\n",
      "[17, 40] loss: 0.142\n",
      "accuracy on valid set on epoch 17 ：95.393%\n",
      "Change learning rate to : 0.000000000 at epoch 18\n",
      "[18, 10] loss: 0.179\n",
      "[18, 20] loss: 0.156\n",
      "[18, 30] loss: 0.149\n",
      "[18, 40] loss: 0.148\n",
      "accuracy on valid set on epoch 18 ：91.506%\n",
      "[19, 10] loss: 0.164\n",
      "[19, 20] loss: 0.162\n",
      "[19, 30] loss: 0.147\n",
      "[19, 40] loss: 0.124\n",
      "accuracy on valid set on epoch 19 ：95.713%\n",
      "Change learning rate to : 0.000000000 at epoch 20\n",
      "[20, 10] loss: 0.151\n",
      "[20, 20] loss: 0.132\n",
      "[20, 30] loss: 0.132\n",
      "[20, 40] loss: 0.146\n",
      "accuracy on valid set on epoch 20 ：92.388%\n",
      "Training Finished, TotalEPOCH=20\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "print(\"Start Training, LeNet!\") \n",
    "for epoch in range(EPOCH):\n",
    "    if epoch % drop_lr_after_epoch_num == (drop_lr_after_epoch_num - 1):\n",
    "            LR_LeNet = LR_LeNet * 0.1\n",
    "            print('Change learning rate to : %.09f at epoch %d' % (LR_LeNet, epoch + 1))\n",
    "\n",
    "\n",
    "    sum_loss = 0.0\n",
    "    # read data\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # clean grad\n",
    "        optimizer_LeNet.zero_grad()\n",
    "\n",
    "        # forward + backward\n",
    "        outputs = my_LeNet(inputs)\n",
    "        loss = criterion_LeNet(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_LeNet.step()\n",
    "\n",
    "        # print everage loss every 10 batch\n",
    "        sum_loss += loss.item()\n",
    "        if i % print_freq == (print_freq - 1):\n",
    "            print('[%d, %d] loss: %.03f'\n",
    "                  % (epoch + 1, i + 1, sum_loss / print_freq))\n",
    "            sum_loss = 0.0\n",
    "    # val upon every epoch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data in valid_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = my_LeNet(images)\n",
    "            # find index\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum()\n",
    "        print('accuracy on valid set on epoch %d ：%.03f%%' % (epoch + 1, (100.0 * correct / total)))\n",
    "\n",
    "torch.save(my_LeNet.state_dict(), 'my_LeNet_%03d.pth' % (epoch + 1))\n",
    "print(\"Training Finished, TotalEPOCH=%d\" % EPOCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of my_LeNet on final test set ：91.640%\n"
     ]
    }
   ],
   "source": [
    "# final test\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = my_LeNet(images)\n",
    "        # find index\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "    print('accuracy of my_LeNet on final test set ：%.03f%%' % (100.0 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN from transfered learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load & modify pretrained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained network and paras\n",
    "my_resnet18 = torchvision.models.resnet18(pretrained=True) \n",
    "\n",
    "# modify last fc layer to 5 class\n",
    "fc_features = my_resnet18.fc.in_features \n",
    "my_resnet18.fc = nn.Linear(fc_features, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fix para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move model to GPU if possible\n",
    "my_resnet18.to(device)\n",
    "\n",
    "# fix para & grad for pretrained layers\n",
    "para_optim = []\n",
    "for k in my_resnet18.children():\n",
    "    if k == my_resnet18.fc:\n",
    "        for param in k.parameters():\n",
    "            para_optim.append(param)\n",
    "    else:\n",
    "        for param in k.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-para\n",
    "EPOCH = 20\n",
    "LR_resnet = 0.001\n",
    "\n",
    "# print setting\n",
    "print_freq = 10\n",
    "drop_lr_after_epoch_num = 2\n",
    "\n",
    "\n",
    "# training setting\n",
    "optimizer_resnet = optim.Adam(para_optim, LR_resnet)\n",
    "criterion_resnet = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training, Resnet-18!\n",
      "[1, 10] loss: 1.554\n",
      "[1, 20] loss: 1.084\n",
      "[1, 30] loss: 0.822\n",
      "[1, 40] loss: 0.689\n",
      "accuracy on valid set on epoch 1 ：80.048%\n",
      "[2, 10] loss: 0.604\n",
      "[2, 20] loss: 0.535\n",
      "[2, 30] loss: 0.489\n",
      "[2, 40] loss: 0.456\n",
      "accuracy on valid set on epoch 2 ：85.697%\n",
      "[3, 10] loss: 0.443\n",
      "[3, 20] loss: 0.420\n",
      "[3, 30] loss: 0.418\n",
      "[3, 40] loss: 0.423\n",
      "accuracy on valid set on epoch 3 ：86.378%\n",
      "Change learning rate to : 0.000100000 at epoch 4\n",
      "[4, 10] loss: 0.390\n",
      "[4, 20] loss: 0.374\n",
      "[4, 30] loss: 0.367\n",
      "[4, 40] loss: 0.413\n",
      "accuracy on valid set on epoch 4 ：88.301%\n",
      "[5, 10] loss: 0.368\n",
      "[5, 20] loss: 0.345\n",
      "[5, 30] loss: 0.336\n",
      "[5, 40] loss: 0.333\n",
      "accuracy on valid set on epoch 5 ：88.462%\n",
      "[6, 10] loss: 0.332\n",
      "[6, 20] loss: 0.325\n",
      "[6, 30] loss: 0.333\n",
      "[6, 40] loss: 0.377\n",
      "accuracy on valid set on epoch 6 ：89.303%\n",
      "[7, 10] loss: 0.340\n",
      "[7, 20] loss: 0.317\n",
      "[7, 30] loss: 0.323\n",
      "[7, 40] loss: 0.301\n",
      "accuracy on valid set on epoch 7 ：89.744%\n",
      "Change learning rate to : 0.000010000 at epoch 8\n",
      "[8, 10] loss: 0.320\n",
      "[8, 20] loss: 0.313\n",
      "[8, 30] loss: 0.310\n",
      "[8, 40] loss: 0.346\n",
      "accuracy on valid set on epoch 8 ：90.184%\n",
      "[9, 10] loss: 0.328\n",
      "[9, 20] loss: 0.295\n",
      "[9, 30] loss: 0.303\n",
      "[9, 40] loss: 0.339\n",
      "accuracy on valid set on epoch 9 ：89.864%\n",
      "[10, 10] loss: 0.298\n",
      "[10, 20] loss: 0.306\n",
      "[10, 30] loss: 0.297\n",
      "[10, 40] loss: 0.310\n",
      "accuracy on valid set on epoch 10 ：89.503%\n",
      "[11, 10] loss: 0.290\n",
      "[11, 20] loss: 0.300\n",
      "[11, 30] loss: 0.293\n",
      "[11, 40] loss: 0.306\n",
      "accuracy on valid set on epoch 11 ：90.064%\n",
      "Change learning rate to : 0.000001000 at epoch 12\n",
      "[12, 10] loss: 0.285\n",
      "[12, 20] loss: 0.294\n",
      "[12, 30] loss: 0.277\n",
      "[12, 40] loss: 0.341\n",
      "accuracy on valid set on epoch 12 ：90.264%\n",
      "[13, 10] loss: 0.298\n",
      "[13, 20] loss: 0.286\n",
      "[13, 30] loss: 0.285\n",
      "[13, 40] loss: 0.293\n",
      "accuracy on valid set on epoch 13 ：90.144%\n",
      "[14, 10] loss: 0.282\n",
      "[14, 20] loss: 0.273\n",
      "[14, 30] loss: 0.279\n",
      "[14, 40] loss: 0.338\n",
      "accuracy on valid set on epoch 14 ：90.184%\n",
      "[15, 10] loss: 0.288\n",
      "[15, 20] loss: 0.281\n",
      "[15, 30] loss: 0.279\n",
      "[15, 40] loss: 0.320\n",
      "accuracy on valid set on epoch 15 ：90.224%\n",
      "Change learning rate to : 0.000000100 at epoch 16\n",
      "[16, 10] loss: 0.288\n",
      "[16, 20] loss: 0.278\n",
      "[16, 30] loss: 0.275\n",
      "[16, 40] loss: 0.276\n",
      "accuracy on valid set on epoch 16 ：90.785%\n",
      "[17, 10] loss: 0.287\n",
      "[17, 20] loss: 0.266\n",
      "[17, 30] loss: 0.278\n",
      "[17, 40] loss: 0.266\n",
      "accuracy on valid set on epoch 17 ：91.306%\n",
      "[18, 10] loss: 0.278\n",
      "[18, 20] loss: 0.272\n",
      "[18, 30] loss: 0.271\n",
      "[18, 40] loss: 0.288\n",
      "accuracy on valid set on epoch 18 ：91.026%\n",
      "[19, 10] loss: 0.281\n",
      "[19, 20] loss: 0.255\n",
      "[19, 30] loss: 0.254\n",
      "[19, 40] loss: 0.260\n",
      "accuracy on valid set on epoch 19 ：90.785%\n",
      "Change learning rate to : 0.000000010 at epoch 20\n",
      "[20, 10] loss: 0.256\n",
      "[20, 20] loss: 0.257\n",
      "[20, 30] loss: 0.240\n",
      "[20, 40] loss: 0.251\n",
      "accuracy on valid set on epoch 20 ：90.785%\n",
      "Training Finished, TotalEPOCH=20\n"
     ]
    }
   ],
   "source": [
    "print_freq = 10\n",
    "drop_lr_after_epoch_num = 4\n",
    "print(\"Start Training, Resnet-18!\") \n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    if epoch % drop_lr_after_epoch_num == (drop_lr_after_epoch_num - 1):\n",
    "            LR_resnet = LR_resnet * 0.1\n",
    "            print('Change learning rate to : %.09f at epoch %d' % (LR_resnet, epoch + 1))\n",
    "\n",
    "    # set to train mode for batch_norm layer\n",
    "    my_resnet18.train()\n",
    "\n",
    "    sum_loss = 0.0\n",
    "    # read data\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # clean grad\n",
    "        optimizer_resnet.zero_grad()\n",
    "\n",
    "        # forward + backward\n",
    "        outputs = my_resnet18(inputs)\n",
    "        loss = criterion_resnet(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_resnet.step()\n",
    "\n",
    "        # print everage loss every 10 batch\n",
    "        sum_loss += loss.item()\n",
    "        if i % print_freq == (print_freq - 1):\n",
    "            print('[%d, %d] loss: %.03f'\n",
    "                  % (epoch + 1, i + 1, sum_loss / print_freq))\n",
    "            sum_loss = 0.0\n",
    "\n",
    "    # val upon every epoch\n",
    "    my_resnet18.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data in valid_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = my_resnet18(images)\n",
    "            # find index\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum()\n",
    "        print('accuracy on valid set on epoch %d ：%.03f%%' % (epoch + 1, (100.0 * correct / total)))\n",
    "\n",
    "torch.save(my_resnet18.state_dict(), 'resnet18_%03d.pth' % (epoch + 1))\n",
    "print(\"Training Finished, TotalEPOCH=%d\" % EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of my_resnet18 on final test set ：89.760%\n"
     ]
    }
   ],
   "source": [
    "# final test\n",
    "my_resnet18.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = my_resnet18(images)\n",
    "        # find index\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "    print('accuracy of my_resnet18 on final test set ：%.03f%%' % (100.0 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
